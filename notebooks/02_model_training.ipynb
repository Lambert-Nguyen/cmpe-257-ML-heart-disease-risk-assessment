{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Model Training\n",
    "## Team: Vi Thi Tuong Nguyen, Lam Nguyen, James Pham, Le Duy Vu\n",
    "\n",
    "This notebook implements:\n",
    "1. **Hierarchical Classification Approach**\n",
    "   - Stage 1: Binary Classification (Disease vs No Disease)\n",
    "   - Stage 2: Multi-class Classification (Severity Levels 1-4)\n",
    "2. **Multiple ML Algorithms** (Random Forest, XGBoost, SVM, Logistic Regression, Gradient Boosting)\n",
    "3. **Hyperparameter Tuning** with GridSearchCV/RandomizedSearchCV\n",
    "4. **Class Imbalance Handling** (SMOTE, class weights, threshold tuning)\n",
    "5. **Ensemble Methods** (Voting, Stacking)\n",
    "6. **Comprehensive Evaluation** (F1-score focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score, \n",
    "                             precision_score, recall_score, accuracy_score,\n",
    "                             roc_auc_score, roc_curve, precision_recall_curve)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load binary classification data\n",
    "X_train_bin = pd.read_csv('../data/processed/X_train_binary.csv')\n",
    "X_test_bin = pd.read_csv('../data/processed/X_test_binary.csv')\n",
    "y_train_bin = pd.read_csv('../data/processed/y_train_binary.csv').values.ravel()\n",
    "y_test_bin = pd.read_csv('../data/processed/y_test_binary.csv').values.ravel()\n",
    "\n",
    "# Load multi-class classification data\n",
    "X_train_multi = pd.read_csv('../data/processed/X_train_multiclass.csv')\n",
    "X_test_multi = pd.read_csv('../data/processed/X_test_multiclass.csv')\n",
    "y_train_multi = pd.read_csv('../data/processed/y_train_multiclass.csv').values.ravel()\n",
    "y_test_multi = pd.read_csv('../data/processed/y_test_multiclass.csv').values.ravel()\n",
    "\n",
    "print(\"Binary Classification Data:\")\n",
    "print(f\"X_train: {X_train_bin.shape}, y_train: {y_train_bin.shape}\")\n",
    "print(f\"X_test: {X_test_bin.shape}, y_test: {y_test_bin.shape}\")\n",
    "print(f\"Class distribution - Train: {Counter(y_train_bin)}\")\n",
    "print(f\"Class distribution - Test: {Counter(y_test_bin)}\")\n",
    "\n",
    "print(\"\\nMulti-class Classification Data:\")\n",
    "print(f\"X_train: {X_train_multi.shape}, y_train: {y_train_multi.shape}\")\n",
    "print(f\"X_test: {X_test_multi.shape}, y_test: {y_test_multi.shape}\")\n",
    "print(f\"Class distribution - Train: {Counter(y_train_multi)}\")\n",
    "print(f\"Class distribution - Test: {Counter(y_test_multi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Class Imbalance with SMOTE and BorderlineSMOTE\n",
    "\n",
    "**SMOTE (Binary Classification)**:\n",
    "- Standard SMOTE works well for binary classification\n",
    "- Creates synthetic samples in feature space\n",
    "- k=5 neighbors for stable synthetic sample generation\n",
    "\n",
    "**BorderlineSMOTE (Multi-class Classification)**:\n",
    "- Focuses on borderline/difficult cases between classes\n",
    "- Better for multi-class problems with severe imbalance\n",
    "- `kind='borderline-1'`: Only creates synthetic samples from minority class borderline cases\n",
    "- More conservative than standard SMOTE\n",
    "- Helps prevent over-generalization in overlapping class regions\n",
    "- Particularly effective for rare severity levels (classes 3-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to binary classification\n",
    "print(\"Applying SMOTE to Binary Classification...\")\n",
    "smote_bin = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)\n",
    "X_train_bin_smote, y_train_bin_smote = smote_bin.fit_resample(X_train_bin, y_train_bin)\n",
    "\n",
    "print(f\"Before SMOTE: {Counter(y_train_bin)}\")\n",
    "print(f\"After SMOTE: {Counter(y_train_bin_smote)}\")\n",
    "print(f\"Shape before: {X_train_bin.shape}, After: {X_train_bin_smote.shape}\")\n",
    "\n",
    "# Apply SMOTE to multi-class (only to disease cases - classes 1-4)\n",
    "print(\"\\nApplying BorderlineSMOTE to Multi-class Classification...\")\n",
    "# BorderlineSMOTE focuses on borderline cases, better for multi-class with imbalance\n",
    "smote_multi = BorderlineSMOTE(random_state=RANDOM_STATE, k_neighbors=3, kind='borderline-1')\n",
    "X_train_multi_smote, y_train_multi_smote = smote_multi.fit_resample(X_train_multi, y_train_multi)\n",
    "\n",
    "print(f\"Before BorderlineSMOTE: {Counter(y_train_multi)}\")\n",
    "print(f\"After BorderlineSMOTE: {Counter(y_train_multi_smote)}\")\n",
    "print(f\"Shape before: {X_train_multi.shape}, After: {X_train_multi_smote.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, is_binary=True):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "        'train_f1': f1_score(y_train, y_pred_train, average='weighted'),\n",
    "        'test_f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if is_binary:\n",
    "        results['test_f1_macro'] = f1_score(y_test, y_pred, average='macro')\n",
    "        results['test_f1_binary'] = f1_score(y_test, y_pred, average='binary')\n",
    "        \n",
    "        # ROC-AUC for binary\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            results['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return results, y_pred\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, labels=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels if labels else 'auto',\n",
    "                yticklabels=labels if labels else 'auto')\n",
    "    plt.title(title, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=20, title=\"Feature Importance\"):\n",
    "    \"\"\"\n",
    "    Plot feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[-top_n:]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(range(len(indices)), importances[indices], color='steelblue')\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(title, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'feature': [feature_names[i] for i in indices],\n",
    "            'importance': importances[indices]\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: Binary Classification (Disease vs No Disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "binary_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "    'SVM': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Train and evaluate baseline models\n",
    "binary_results = []\n",
    "\n",
    "print(\"Training Binary Classification Models (with SMOTE)...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in binary_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train on SMOTE data\n",
    "    model.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, y_pred = evaluate_model(\n",
    "        model, X_train_bin_smote, y_train_bin_smote, \n",
    "        X_test_bin, y_test_bin, name, is_binary=True\n",
    "    )\n",
    "    binary_results.append(results)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 (weighted): {results['test_f1']:.4f}\")\n",
    "    print(f\"  Test F1 (binary): {results['test_f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {results.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "binary_results_df = pd.DataFrame(binary_results)\n",
    "binary_results_df = binary_results_df.sort_values('test_f1', ascending=False)\n",
    "\n",
    "print(\"\\nBinary Classification - Baseline Results:\")\n",
    "print(binary_results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "binary_results_df.plot(x='model', y=['test_accuracy', 'test_f1'], kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Binary Classification: Accuracy vs F1-Score', fontweight='bold')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend(['Accuracy', 'F1-Score (weighted)'])\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "binary_results_df.plot(x='model', y=['precision', 'recall'], kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Binary Classification: Precision vs Recall', fontweight='bold')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter Tuning for Best Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 2 models for tuning\n",
    "top_binary_models = binary_results_df.head(2)['model'].tolist()\n",
    "print(f\"Top models for tuning: {top_binary_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"Tuning Random Forest...\")\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    rf_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_random_search.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_random_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {rf_random_search.best_score_:.4f}\")\n",
    "\n",
    "best_rf_bin = rf_random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "print(\"Tuning XGBoost...\")\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'scale_pos_weight': [1, 2, 3]\n",
    "}\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "    xgb_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_random_search.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "\n",
    "print(f\"\\nBest parameters: {xgb_random_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {xgb_random_search.best_score_:.4f}\")\n",
    "\n",
    "best_xgb_bin = xgb_random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models\n",
    "print(\"\\nEvaluating Tuned Models...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tuned_results = []\n",
    "\n",
    "for name, model in [('Random Forest (Tuned)', best_rf_bin), ('XGBoost (Tuned)', best_xgb_bin)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    results, y_pred = evaluate_model(\n",
    "        model, X_train_bin_smote, y_train_bin_smote,\n",
    "        X_test_bin, y_test_bin, name, is_binary=True\n",
    "    )\n",
    "    tuned_results.append(results)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 (weighted): {results['test_f1']:.4f}\")\n",
    "    print(f\"  Test F1 (binary): {results['test_f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {results.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(y_test_bin, y_pred, f'{name} - Confusion Matrix', \n",
    "                         labels=['No Disease', 'Disease'])\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_imp = plot_feature_importance(model, X_train_bin.columns, \n",
    "                                         title=f'{name} - Top 20 Features')\n",
    "    if feature_imp is not None:\n",
    "        print(f\"\\nTop 10 Important Features:\")\n",
    "        print(feature_imp.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ensemble Methods for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "print(\"Training Voting Ensemble...\")\n",
    "\n",
    "voting_clf_bin = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf_bin),\n",
    "        ('xgb', best_xgb_bin),\n",
    "        ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf_bin.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "\n",
    "results_voting, y_pred_voting = evaluate_model(\n",
    "    voting_clf_bin, X_train_bin_smote, y_train_bin_smote,\n",
    "    X_test_bin, y_test_bin, 'Voting Ensemble', is_binary=True\n",
    ")\n",
    "\n",
    "print(f\"\\nVoting Ensemble Results:\")\n",
    "print(f\"  Test Accuracy: {results_voting['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1 (weighted): {results_voting['test_f1']:.4f}\")\n",
    "print(f\"  Test F1 (binary): {results_voting['test_f1_binary']:.4f}\")\n",
    "print(f\"  ROC-AUC: {results_voting.get('roc_auc', 'N/A')}\")\n",
    "\n",
    "plot_confusion_matrix(y_test_bin, y_pred_voting, \n",
    "                     'Voting Ensemble - Confusion Matrix',\n",
    "                     labels=['No Disease', 'Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "print(\"Training Stacking Ensemble...\")\n",
    "\n",
    "stacking_clf_bin = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf_bin),\n",
    "        ('xgb', best_xgb_bin),\n",
    "        ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=RANDOM_STATE),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacking_clf_bin.fit(X_train_bin_smote, y_train_bin_smote)\n",
    "\n",
    "results_stacking, y_pred_stacking = evaluate_model(\n",
    "    stacking_clf_bin, X_train_bin_smote, y_train_bin_smote,\n",
    "    X_test_bin, y_test_bin, 'Stacking Ensemble', is_binary=True\n",
    ")\n",
    "\n",
    "print(f\"\\nStacking Ensemble Results:\")\n",
    "print(f\"  Test Accuracy: {results_stacking['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1 (weighted): {results_stacking['test_f1']:.4f}\")\n",
    "print(f\"  Test F1 (binary): {results_stacking['test_f1_binary']:.4f}\")\n",
    "print(f\"  ROC-AUC: {results_stacking.get('roc_auc', 'N/A')}\")\n",
    "\n",
    "plot_confusion_matrix(y_test_bin, y_pred_stacking, \n",
    "                     'Stacking Ensemble - Confusion Matrix',\n",
    "                     labels=['No Disease', 'Disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Select Best Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all binary models\n",
    "all_binary_results = tuned_results + [results_voting, results_stacking]\n",
    "all_binary_df = pd.DataFrame(all_binary_results).sort_values('test_f1', ascending=False)\n",
    "\n",
    "print(\"\\nFinal Binary Classification Results:\")\n",
    "print(all_binary_df.to_string(index=False))\n",
    "\n",
    "best_binary_model_name = all_binary_df.iloc[0]['model']\n",
    "best_binary_f1 = all_binary_df.iloc[0]['test_f1']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST BINARY MODEL: {best_binary_model_name}\")\n",
    "print(f\"Test F1-Score: {best_binary_f1:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Select the best model\n",
    "if 'Voting' in best_binary_model_name:\n",
    "    best_binary_model = voting_clf_bin\n",
    "elif 'Stacking' in best_binary_model_name:\n",
    "    best_binary_model = stacking_clf_bin\n",
    "elif 'XGBoost' in best_binary_model_name:\n",
    "    best_binary_model = best_xgb_bin\n",
    "else:\n",
    "    best_binary_model = best_rf_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Multi-class Classification (Severity Levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline Multi-class Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multi-class models\n",
    "multiclass_models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=200, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(random_state=RANDOM_STATE, eval_metric='mlogloss'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(random_state=RANDOM_STATE, kernel='rbf', probability=True)\n",
    "}\n",
    "\n",
    "# Train and evaluate baseline multi-class models\n",
    "multiclass_results = []\n",
    "\n",
    "print(\"Training Multi-class Classification Models (with BorderlineSMOTE)...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in multiclass_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train on SMOTE data\n",
    "    model.fit(X_train_multi_smote, y_train_multi_smote)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, y_pred = evaluate_model(\n",
    "        model, X_train_multi_smote, y_train_multi_smote,\n",
    "        X_test_multi, y_test_multi, name, is_binary=False\n",
    "    )\n",
    "    multiclass_results.append(results)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  Test F1 (weighted): {results['test_f1']:.4f}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n  Classification Report:\")\n",
    "    print(classification_report(y_test_multi, y_pred, \n",
    "                                target_names=[f'Class {i}' for i in range(5)]))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(y_test_multi, y_pred, \n",
    "                         f'{name} - Multi-class Confusion Matrix',\n",
    "                         labels=[f'Level {i}' for i in range(5)])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multi-class baseline models\n",
    "multiclass_results_df = pd.DataFrame(multiclass_results).sort_values('test_f1', ascending=False)\n",
    "\n",
    "print(\"\\nMulti-class Classification - Baseline Results:\")\n",
    "print(multiclass_results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "multiclass_results_df.plot(x='model', y=['test_accuracy', 'test_f1'], kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Multi-class: Accuracy vs F1-Score', fontweight='bold')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "multiclass_results_df.plot(x='model', y=['precision', 'recall'], kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Multi-class: Precision vs Recall', fontweight='bold')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter Tuning for Best Multi-class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top model for tuning\n",
    "top_multiclass_model = multiclass_results_df.head(1)['model'].tolist()[0]\n",
    "print(f\"Top multi-class model for tuning: {top_multiclass_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the best performing model (likely Random Forest or XGBoost)\n",
    "if 'Random Forest' in top_multiclass_model:\n",
    "    print(\"Tuning Random Forest for Multi-class...\")\n",
    "    \n",
    "    rf_param_grid_multi = {\n",
    "        'n_estimators': [200, 300, 400],\n",
    "        'max_depth': [15, 20, 25, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'class_weight': ['balanced', 'balanced_subsample']\n",
    "    }\n",
    "    \n",
    "    rf_search_multi = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        rf_param_grid_multi,\n",
    "        n_iter=40,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    rf_search_multi.fit(X_train_multi_smote, y_train_multi_smote)\n",
    "    best_multi_model = rf_search_multi.best_estimator_\n",
    "    best_cv_score = rf_search_multi.best_score_\n",
    "    \n",
    "elif 'XGBoost' in top_multiclass_model:\n",
    "    print(\"Tuning XGBoost for Multi-class...\")\n",
    "    \n",
    "    xgb_param_grid_multi = {\n",
    "        'n_estimators': [200, 300, 400],\n",
    "        'max_depth': [5, 7, 9, 11],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "    \n",
    "    xgb_search_multi = RandomizedSearchCV(\n",
    "        XGBClassifier(random_state=RANDOM_STATE, eval_metric='mlogloss'),\n",
    "        xgb_param_grid_multi,\n",
    "        n_iter=40,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    xgb_search_multi.fit(X_train_multi_smote, y_train_multi_smote)\n",
    "    best_multi_model = xgb_search_multi.best_estimator_\n",
    "    best_cv_score = xgb_search_multi.best_score_\n",
    "else:\n",
    "    # If neither Random Forest nor XGBoost, tune whichever model is top\n",
    "    print(f\"Tuning {top_multiclass_model} for Multi-class...\")\n",
    "    \n",
    "    if 'Gradient' in top_multiclass_model:\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "        base_model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "    elif 'SVM' in top_multiclass_model:\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "            'kernel': ['rbf', 'poly']\n",
    "        }\n",
    "        base_model = SVC(random_state=RANDOM_STATE, probability=True)\n",
    "    else:\n",
    "        # Fallback to Random Forest\n",
    "        param_grid = {\n",
    "            'n_estimators': [200, 300],\n",
    "            'max_depth': [15, 20, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "        base_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_grid,\n",
    "        n_iter=30,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train_multi_smote, y_train_multi_smote)\n",
    "    best_multi_model = search.best_estimator_\n",
    "    best_cv_score = search.best_score_\n",
    "\n",
    "print(f\"\\nBest cross-validation F1 score: {best_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned multi-class model\n",
    "print(\"\\nEvaluating Tuned Multi-class Model...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_tuned_multi, y_pred_tuned = evaluate_model(\n",
    "    best_multi_model, X_train_multi_smote, y_train_multi_smote,\n",
    "    X_test_multi, y_test_multi, f'{top_multiclass_model} (Tuned)', is_binary=False\n",
    ")\n",
    "\n",
    "print(f\"\\n{top_multiclass_model} (Tuned) Results:\")\n",
    "print(f\"  Test Accuracy: {results_tuned_multi['test_accuracy']:.4f}\")\n",
    "print(f\"  Test F1 (weighted): {results_tuned_multi['test_f1']:.4f}\")\n",
    "print(f\"  Precision: {results_tuned_multi['precision']:.4f}\")\n",
    "print(f\"  Recall: {results_tuned_multi['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Classification Report:\")\n",
    "print(classification_report(y_test_multi, y_pred_tuned,\n",
    "                           target_names=[f'Class {i}' for i in range(5)]))\n",
    "\n",
    "plot_confusion_matrix(y_test_multi, y_pred_tuned,\n",
    "                     f'{top_multiclass_model} (Tuned) - Confusion Matrix',\n",
    "                     labels=[f'Level {i}' for i in range(5)])\n",
    "\n",
    "# Feature importance\n",
    "feature_imp_multi = plot_feature_importance(best_multi_model, X_train_multi.columns,\n",
    "                                           title=f'{top_multiclass_model} (Tuned) - Top 20 Features')\n",
    "if feature_imp_multi is not None:\n",
    "    print(f\"\\nTop 10 Important Features for Multi-class:\")\n",
    "    print(feature_imp_multi.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical classifier\n",
    "class HierarchicalClassifier:\n",
    "    \"\"\"\n",
    "    Two-stage hierarchical classifier:\n",
    "    Stage 1: Binary (Disease vs No Disease)\n",
    "    Stage 2: Multi-class (Severity Level 1-4 for Disease cases ONLY)\n",
    "    \n",
    "    Note: Multi-class model is trained on disease cases only (classes 1-4),\n",
    "    so it never learned class 0. This is optimal for hierarchical classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, binary_model, multiclass_model):\n",
    "        self.binary_model = binary_model\n",
    "        self.multiclass_model = multiclass_model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Stage 1: Binary prediction\n",
    "        binary_pred = self.binary_model.predict(X)\n",
    "        \n",
    "        # Initialize final predictions as 0 (no disease)\n",
    "        final_pred = np.zeros(len(X), dtype=int)\n",
    "        \n",
    "        # Stage 2: For disease cases, predict severity (1-4)\n",
    "        disease_mask = binary_pred == 1\n",
    "        if disease_mask.sum() > 0:\n",
    "            X_disease = X[disease_mask]\n",
    "            # Multi-class predicts 1, 2, 3, or 4 (never 0)\n",
    "            severity_pred = self.multiclass_model.predict(X_disease)\n",
    "            final_pred[disease_mask] = severity_pred\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Get binary probabilities\n",
    "        binary_proba = self.binary_model.predict_proba(X)\n",
    "        \n",
    "        # Initialize final probabilities (5 classes: 0-4)\n",
    "        final_proba = np.zeros((len(X), 5))\n",
    "        \n",
    "        # No disease probability (class 0) comes from binary model\n",
    "        final_proba[:, 0] = binary_proba[:, 0]\n",
    "        \n",
    "        # Disease probability to distribute across severity levels\n",
    "        disease_prob = binary_proba[:, 1]\n",
    "        \n",
    "        # Get multi-class probabilities (for classes 1-4)\n",
    "        # Note: multi-class model outputs 4 classes, not 5\n",
    "        multi_proba = self.multiclass_model.predict_proba(X)\n",
    "        \n",
    "        # Distribute disease probability across severity levels 1-4\n",
    "        # multi_proba has shape (n_samples, 4) for classes 1, 2, 3, 4\n",
    "        for i in range(4):  # 4 severity levels\n",
    "            final_proba[:, i+1] = disease_prob * multi_proba[:, i]\n",
    "        \n",
    "        return final_proba\n",
    "\n",
    "# Initialize hierarchical classifier\n",
    "hierarchical_clf = HierarchicalClassifier(best_binary_model, best_multi_model)\n",
    "\n",
    "print(\"Hierarchical Classifier Created!\")\n",
    "print(f\"Stage 1 (Binary): {best_binary_model_name}\")\n",
    "print(f\"Stage 2 (Multi-class on disease cases 1-4): {top_multiclass_model} (Tuned)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hierarchical classifier\n",
    "print(\"\\nEvaluating Hierarchical Classifier...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_pred_hierarchical = hierarchical_clf.predict(X_test_multi)\n",
    "\n",
    "# Calculate metrics\n",
    "hierarchical_accuracy = accuracy_score(y_test_multi, y_pred_hierarchical)\n",
    "hierarchical_f1 = f1_score(y_test_multi, y_pred_hierarchical, average='weighted')\n",
    "hierarchical_precision = precision_score(y_test_multi, y_pred_hierarchical, average='weighted')\n",
    "hierarchical_recall = recall_score(y_test_multi, y_pred_hierarchical, average='weighted')\n",
    "\n",
    "print(f\"Hierarchical Classifier Results:\")\n",
    "print(f\"  Test Accuracy: {hierarchical_accuracy:.4f}\")\n",
    "print(f\"  Test F1 (weighted): {hierarchical_f1:.4f}\")\n",
    "print(f\"  Precision: {hierarchical_precision:.4f}\")\n",
    "print(f\"  Recall: {hierarchical_recall:.4f}\")\n",
    "\n",
    "print(f\"\\n  Classification Report:\")\n",
    "print(classification_report(y_test_multi, y_pred_hierarchical,\n",
    "                           target_names=[f'Class {i}' for i in range(5)]))\n",
    "\n",
    "plot_confusion_matrix(y_test_multi, y_pred_hierarchical,\n",
    "                     'Hierarchical Classifier - Confusion Matrix',\n",
    "                     labels=[f'Level {i}' for i in range(5)])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "final_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Approach': 'Direct Multi-class (Best Baseline)',\n",
    "        'Model': multiclass_results_df.iloc[0]['model'],\n",
    "        'Test Accuracy': multiclass_results_df.iloc[0]['test_accuracy'],\n",
    "        'Test F1 (weighted)': multiclass_results_df.iloc[0]['test_f1'],\n",
    "        'Precision': multiclass_results_df.iloc[0]['precision'],\n",
    "        'Recall': multiclass_results_df.iloc[0]['recall']\n",
    "    },\n",
    "    {\n",
    "        'Approach': 'Direct Multi-class (Tuned)',\n",
    "        'Model': f'{top_multiclass_model} (Tuned)',\n",
    "        'Test Accuracy': results_tuned_multi['test_accuracy'],\n",
    "        'Test F1 (weighted)': results_tuned_multi['test_f1'],\n",
    "        'Precision': results_tuned_multi['precision'],\n",
    "        'Recall': results_tuned_multi['recall']\n",
    "    },\n",
    "    {\n",
    "        'Approach': 'Hierarchical',\n",
    "        'Model': f'Binary: {best_binary_model_name} + Multi: {top_multiclass_model}',\n",
    "        'Test Accuracy': hierarchical_accuracy,\n",
    "        'Test F1 (weighted)': hierarchical_f1,\n",
    "        'Precision': hierarchical_precision,\n",
    "        'Recall': hierarchical_recall\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize final comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(final_comparison))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width*1.5, final_comparison['Test Accuracy'], width, label='Accuracy')\n",
    "ax.bar(x - width/2, final_comparison['Test F1 (weighted)'], width, label='F1-Score')\n",
    "ax.bar(x + width/2, final_comparison['Precision'], width, label='Precision')\n",
    "ax.bar(x + width*1.5, final_comparison['Recall'], width, label='Recall')\n",
    "\n",
    "ax.set_xlabel('Approach')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Final Model Comparison - All Metrics', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(final_comparison['Approach'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best overall model\n",
    "best_overall_idx = final_comparison['Test F1 (weighted)'].idxmax()\n",
    "best_overall = final_comparison.iloc[best_overall_idx]\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"BEST OVERALL MODEL: {best_overall['Approach']}\")\n",
    "print(f\"Model: {best_overall['Model']}\")\n",
    "print(f\"Test F1-Score (weighted): {best_overall['Test F1 (weighted)']:.4f}\")\n",
    "print(f\"Test Accuracy: {best_overall['Test Accuracy']:.4f}\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all important models\n",
    "models_to_save = {\n",
    "    'best_binary_model': best_binary_model,\n",
    "    'best_multiclass_model': best_multi_model,\n",
    "    'hierarchical_classifier': hierarchical_clf,\n",
    "    'smote_binary': smote_bin,\n",
    "    'smote_multiclass': smote_multi\n",
    "}\n",
    "\n",
    "for model_name, model in models_to_save.items():\n",
    "    with open(f'../models/{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Saved: {model_name}.pkl\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_binary_model_name': best_binary_model_name,\n",
    "    'best_binary_f1': best_binary_f1,\n",
    "    'best_multiclass_model_name': top_multiclass_model,\n",
    "    'best_multiclass_f1': results_tuned_multi['test_f1'],\n",
    "    'hierarchical_f1': hierarchical_f1,\n",
    "    'best_overall_approach': best_overall['Approach'],\n",
    "    'feature_names': X_train_multi.columns.tolist(),\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'smote_method_binary': 'SMOTE',\n",
    "    'smote_method_multiclass': 'BorderlineSMOTE (kind=borderline-1)'\n",
    "}\n",
    "\n",
    "with open('../models/model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"\\nModel metadata saved!\")\n",
    "print(f\"\\nAll models saved to: ../models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"HEART DISEASE PREDICTION - TRAINING SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n1. BINARY CLASSIFICATION (Disease Detection)\")\n",
    "print(f\"   Best Model: {best_binary_model_name}\")\n",
    "print(f\"   Test F1-Score: {best_binary_f1:.4f}\")\n",
    "print(f\"   Models Trained: {len(binary_models)} baseline + {len(tuned_results)} tuned + 2 ensemble\")\n",
    "\n",
    "print(f\"\\n2. MULTI-CLASS CLASSIFICATION (Severity Assessment)\")\n",
    "print(f\"   Best Model: {top_multiclass_model} (Tuned)\")\n",
    "print(f\"   Test F1-Score: {results_tuned_multi['test_f1']:.4f}\")\n",
    "print(f\"   Models Trained: {len(multiclass_models)} baseline + 1 tuned\")\n",
    "\n",
    "print(f\"\\n3. HIERARCHICAL CLASSIFICATION\")\n",
    "print(f\"   Stage 1: {best_binary_model_name}\")\n",
    "print(f\"   Stage 2: {top_multiclass_model} (Tuned)\")\n",
    "print(f\"   Test F1-Score: {hierarchical_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n4. BEST OVERALL APPROACH\")\n",
    "print(f\"   Approach: {best_overall['Approach']}\")\n",
    "print(f\"   Test F1-Score: {best_overall['Test F1 (weighted)']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_overall['Test Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n5. TECHNIQUES USED\")\n",
    "print(f\"   - Class Imbalance (Binary): SMOTE oversampling\")\n",
    "print(f\"   - Class Imbalance (Multi-class): BorderlineSMOTE (borderline-1)\")\n",
    "print(f\"   - Hyperparameter Tuning: RandomizedSearchCV with 5-fold CV\")\n",
    "print(f\"   - Ensemble Methods: Voting, Stacking\")\n",
    "print(f\"   - Evaluation: Stratified K-Fold, weighted F1-score\")\n",
    "\n",
    "print(f\"\\n6. MODELS SAVED\")\n",
    "print(f\"   - best_binary_model.pkl\")\n",
    "print(f\"   - best_multiclass_model.pkl\")\n",
    "print(f\"   - hierarchical_classifier.pkl\")\n",
    "print(f\"   - model_metadata.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TRAINING COMPLETE! Ready for deployment.\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
